{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "박막 두께 측정_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcR9hn1KodekXDASP2AgYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/layer_thickness/blob/master/%EB%B0%95%EB%A7%89_%EB%91%90%EA%BB%98_%EC%B8%A1%EC%A0%95_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wHz4rdLwRH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "033f7974-d202-4c74-e34d-784334c3c5e6"
      },
      "source": [
        "# 필요한 라이브러리 import\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 데이터 처리\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import * # 일단 싹 다. 모델평가를 해야해서...\n",
        "from sklearn.model_selection import * # 너도 싹 다.\n",
        "import tensorflow.keras\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uud-jSgvnX3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/4uiiurz1/keras-cosine-annealing/blob/master/cosine_annealing.py\n",
        "# pytorch와 달리 tensorflow의 keras에는 cosine anneling이 구현되어 있지 않아서 직접 구현해야함\n",
        "# open source활용\n",
        "import math\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class CosineAnnealingScheduler(Callback):\n",
        "    \"\"\"Cosine annealing scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
        "        super(CosineAnnealingScheduler, self).__init__()\n",
        "        self.T_max = T_max\n",
        "        self.eta_max = eta_max\n",
        "        self.eta_min = eta_min\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "        if self.verbose > 0:\n",
        "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
        "                  'rate to %s.' % (epoch + 1, lr))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCQbLmhzL0KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation fucntion으로 gelu를 사용 gelu가 relu보다 효과가 좋음\n",
        "# Gaussian Error Linear Units\n",
        "class Gelu(Activation):\n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(Gelu, self).__init__(activation, **kwargs)\n",
        "        self.__name__='gelu'\n",
        "        \n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "\n",
        "get_custom_objects().update({'gelu': Gelu(gelu)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoYyfU7IL33o",
        "colab_type": "code",
        "outputId": "b054c483-a792-446f-fe6d-dbe21f2a5b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# 구글 드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-_48EsMJra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 불러오기\n",
        "train = pd.read_csv('/content/gdrive/My Drive/machine_learning/박막두께/train_set.csv', index_col = 0)\n",
        "test = pd.read_csv('/content/gdrive/My Drive/machine_learning/박막두께/test_set.csv', index_col = 0)\n",
        "sample_submission = pd.read_csv('/content/gdrive/My Drive/machine_learning/박막두께/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr6gX8a1MNh5",
        "colab_type": "code",
        "outputId": "71986d59-21c4-4034-e77f-cb6f67f3068c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "# 데이터 요약\n",
        "print(train.describe())\n",
        "print(test.describe())\n",
        "print(sample_submission.describe())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             layer_1        layer_2  ...            224            225\n",
            "count  170100.000000  170100.000000  ...  170100.000000  170100.000000\n",
            "mean      155.043269     155.010053  ...       0.631455       0.633718\n",
            "std        86.527234      86.602764  ...       0.195622       0.195101\n",
            "min        10.000000      10.000000  ...      -0.005321      -0.005659\n",
            "25%        80.000000      80.000000  ...       0.508407       0.511000\n",
            "50%       160.000000     160.000000  ...       0.677139       0.679399\n",
            "75%       230.000000     230.000000  ...       0.787409       0.789263\n",
            "max       300.000000     300.000000  ...       0.941404       0.943648\n",
            "\n",
            "[8 rows x 230 columns]\n",
            "                  0             1  ...           224           225\n",
            "count  18900.000000  18900.000000  ...  18900.000000  18900.000000\n",
            "mean       0.297972      0.298041  ...      0.632419      0.634869\n",
            "std        0.182991      0.183320  ...      0.194512      0.194490\n",
            "min       -0.013451     -0.013265  ...     -0.001584     -0.005085\n",
            "25%        0.138633      0.140467  ...      0.514230      0.517084\n",
            "50%        0.292453      0.292547  ...      0.677810      0.680802\n",
            "75%        0.443753      0.443661  ...      0.786448      0.788634\n",
            "max        0.739840      0.738960  ...      0.934890      0.939436\n",
            "\n",
            "[8 rows x 226 columns]\n",
            "                id  layer_1  layer_2  layer_3  layer_4\n",
            "count  18900.00000  18900.0  18900.0  18900.0  18900.0\n",
            "mean    9449.50000      0.0      0.0      0.0      0.0\n",
            "std     5456.10438      0.0      0.0      0.0      0.0\n",
            "min        0.00000      0.0      0.0      0.0      0.0\n",
            "25%     4724.75000      0.0      0.0      0.0      0.0\n",
            "50%     9449.50000      0.0      0.0      0.0      0.0\n",
            "75%    14174.25000      0.0      0.0      0.0      0.0\n",
            "max    18899.00000      0.0      0.0      0.0      0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLbeIZ6cMOfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 슬라이싱을 통한 변수분리\n",
        "## 자료형은 pandas가 아닌 numpy\n",
        "train_X = np.array(train.iloc[:, 4:]) \n",
        "train_Y = np.array(train.iloc[:,0:4])\n",
        "test_X = np.array(test.iloc[:, 0:]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKcs50kZMU2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델을 평가하기 위한 데이터분리 (비율은 10%)\n",
        "# random_state는 튜닝가능한 하이퍼 파라미터?\n",
        "\n",
        "x_train, x_eval, y_train, y_eval = train_test_split(train_X, train_Y, test_size = 0.1, random_state = 52)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzk0OVkUpozC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSoJQvzxMPxy",
        "colab_type": "code",
        "outputId": "e4debbc5-f68a-4eb1-afd1-23f90661072d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "drop_ratio = 0.0001\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2017, input_dim = len(x_train[0])))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Dense(2013))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Dense(1027))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Dense(517))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Dense(509))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Dense(503))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4))\n",
        "model.add(Activation(gelu))\n",
        "model.add(Dropout(drop_ratio))\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 2017)              457859    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 2017)              8068      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 2017)              0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 2017)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 2013)              4062234   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 2013)              8052      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 2013)              0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 2013)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1027)              2068378   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 1027)              4108      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 1027)              0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 1027)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 517)               531476    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 517)               2068      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 517)               0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 517)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 509)               263662    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 509)               2036      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 509)               0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 509)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 503)               256530    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 503)               2012      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 503)               0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 503)               0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 503)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 4)                 2016      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 7,668,499\n",
            "Trainable params: 7,655,327\n",
            "Non-trainable params: 13,172\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkWLc4dniKIj",
        "colab_type": "code",
        "outputId": "02cd1771-0329-4087-c74a-204cb6200d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "lr = 3e-4\n",
        "lr_d = 0.0\n",
        "patience = 200\n",
        "\n",
        "n_fold = 13\n",
        "k_fold = KFold(n_splits = n_fold, shuffle = True, random_state = 1000).split(x_train, y_train)  \n",
        "\n",
        "for k, (train, test) in enumerate(k_fold):\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-36cc31807735>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSywf_rIMSjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mae', optimizer= 'nadam', metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56FMbut8M8Wo",
        "colab_type": "code",
        "outputId": "2e4113ae-7fa7-4fb9-88a5-7a222cfa802e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size = 4096, epochs = 1000, validation_split = 0.05, verbose = 1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 150.3865 - mae: 150.3865 - val_loss: 149.3890 - val_mae: 149.3890\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 144.2206 - mae: 144.2206 - val_loss: 131.0239 - val_mae: 131.0239\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 135.9227 - mae: 135.9227 - val_loss: 110.4232 - val_mae: 110.4232\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 124.5896 - mae: 124.5896 - val_loss: 107.4090 - val_mae: 107.4090\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 110.9609 - mae: 110.9609 - val_loss: 127.2086 - val_mae: 127.2086\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 96.1869 - mae: 96.1869 - val_loss: 134.1345 - val_mae: 134.1345\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 81.6227 - mae: 81.6227 - val_loss: 131.7654 - val_mae: 131.7654\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 68.0373 - mae: 68.0373 - val_loss: 123.1589 - val_mae: 123.1589\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 56.4337 - mae: 56.4337 - val_loss: 97.2250 - val_mae: 97.2250\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 44.7531 - mae: 44.7531 - val_loss: 91.5789 - val_mae: 91.5789\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 33.7815 - mae: 33.7815 - val_loss: 69.3139 - val_mae: 69.3139\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 25.6806 - mae: 25.6806 - val_loss: 58.8311 - val_mae: 58.8311\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 20.1762 - mae: 20.1762 - val_loss: 41.6077 - val_mae: 41.6077\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 15.8400 - mae: 15.8400 - val_loss: 33.7185 - val_mae: 33.7185\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 13.0211 - mae: 13.0211 - val_loss: 29.4933 - val_mae: 29.4933\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 10.9738 - mae: 10.9738 - val_loss: 20.6270 - val_mae: 20.6270\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 9.5541 - mae: 9.5541 - val_loss: 16.8793 - val_mae: 16.8793\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 8.6957 - mae: 8.6957 - val_loss: 14.6942 - val_mae: 14.6942\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 7.8829 - mae: 7.8829 - val_loss: 12.5202 - val_mae: 12.5202\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 7.1881 - mae: 7.1881 - val_loss: 10.2689 - val_mae: 10.2689\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 6.7151 - mae: 6.7151 - val_loss: 9.0992 - val_mae: 9.0992\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 6.3088 - mae: 6.3088 - val_loss: 7.9494 - val_mae: 7.9494\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 5.9067 - mae: 5.9067 - val_loss: 7.2526 - val_mae: 7.2526\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 5.6935 - mae: 5.6935 - val_loss: 7.1800 - val_mae: 7.1800\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 5.4232 - mae: 5.4232 - val_loss: 7.0418 - val_mae: 7.0418\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 5.1795 - mae: 5.1795 - val_loss: 6.4113 - val_mae: 6.4113\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 4.9535 - mae: 4.9535 - val_loss: 6.4568 - val_mae: 6.4568\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 4.7978 - mae: 4.7978 - val_loss: 5.9342 - val_mae: 5.9342\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 4.6006 - mae: 4.6006 - val_loss: 5.5646 - val_mae: 5.5646\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 4.5641 - mae: 4.5641 - val_loss: 5.4330 - val_mae: 5.4330\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 4.3138 - mae: 4.3138 - val_loss: 4.8221 - val_mae: 4.8221\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 4.2250 - mae: 4.2250 - val_loss: 5.2717 - val_mae: 5.2717\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 4.1207 - mae: 4.1207 - val_loss: 4.5908 - val_mae: 4.5908\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 4.0869 - mae: 4.0869 - val_loss: 5.0362 - val_mae: 5.0362\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 3.9925 - mae: 3.9925 - val_loss: 4.8041 - val_mae: 4.8041\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.8934 - mae: 3.8934 - val_loss: 4.6732 - val_mae: 4.6732\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.8414 - mae: 3.8414 - val_loss: 4.6741 - val_mae: 4.6741\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.6540 - mae: 3.6540 - val_loss: 4.0796 - val_mae: 4.0796\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.6365 - mae: 3.6365 - val_loss: 3.9135 - val_mae: 3.9135\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 3.6078 - mae: 3.6078 - val_loss: 4.1485 - val_mae: 4.1485\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.5196 - mae: 3.5196 - val_loss: 3.9038 - val_mae: 3.9038\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.4471 - mae: 3.4471 - val_loss: 4.0138 - val_mae: 4.0138\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 3.3748 - mae: 3.3748 - val_loss: 3.6507 - val_mae: 3.6507\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.3193 - mae: 3.3193 - val_loss: 3.7994 - val_mae: 3.7994\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.2750 - mae: 3.2750 - val_loss: 3.5490 - val_mae: 3.5490\n",
            "Epoch 46/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.2342 - mae: 3.2342 - val_loss: 3.8055 - val_mae: 3.8055\n",
            "Epoch 47/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.1801 - mae: 3.1801 - val_loss: 3.5206 - val_mae: 3.5206\n",
            "Epoch 48/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 3.1415 - mae: 3.1415 - val_loss: 3.3449 - val_mae: 3.3449\n",
            "Epoch 49/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 3.1047 - mae: 3.1047 - val_loss: 3.2771 - val_mae: 3.2771\n",
            "Epoch 50/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.9941 - mae: 2.9941 - val_loss: 3.5177 - val_mae: 3.5177\n",
            "Epoch 51/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 3.0587 - mae: 3.0587 - val_loss: 3.0950 - val_mae: 3.0950\n",
            "Epoch 52/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 3.0082 - mae: 3.0082 - val_loss: 2.8947 - val_mae: 2.8947\n",
            "Epoch 53/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.9504 - mae: 2.9504 - val_loss: 3.3460 - val_mae: 3.3460\n",
            "Epoch 54/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.8851 - mae: 2.8851 - val_loss: 3.0823 - val_mae: 3.0823\n",
            "Epoch 55/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.8305 - mae: 2.8305 - val_loss: 3.0126 - val_mae: 3.0126\n",
            "Epoch 56/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.8466 - mae: 2.8466 - val_loss: 3.0778 - val_mae: 3.0778\n",
            "Epoch 57/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.7771 - mae: 2.7771 - val_loss: 3.0368 - val_mae: 3.0368\n",
            "Epoch 58/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.8476 - mae: 2.8476 - val_loss: 3.0311 - val_mae: 3.0311\n",
            "Epoch 59/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.7114 - mae: 2.7114 - val_loss: 2.7417 - val_mae: 2.7417\n",
            "Epoch 60/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 2.7434 - mae: 2.7434 - val_loss: 2.8832 - val_mae: 2.8832\n",
            "Epoch 61/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6539 - mae: 2.6539 - val_loss: 2.8712 - val_mae: 2.8712\n",
            "Epoch 62/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6948 - mae: 2.6948 - val_loss: 3.1625 - val_mae: 3.1625\n",
            "Epoch 63/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.7439 - mae: 2.7439 - val_loss: 2.7386 - val_mae: 2.7386\n",
            "Epoch 64/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6106 - mae: 2.6106 - val_loss: 2.8429 - val_mae: 2.8429\n",
            "Epoch 65/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 2.6549 - mae: 2.6549 - val_loss: 2.8275 - val_mae: 2.8275\n",
            "Epoch 66/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6866 - mae: 2.6866 - val_loss: 2.7798 - val_mae: 2.7798\n",
            "Epoch 67/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6229 - mae: 2.6229 - val_loss: 2.6944 - val_mae: 2.6944\n",
            "Epoch 68/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 2.5875 - mae: 2.5875 - val_loss: 2.8990 - val_mae: 2.8990\n",
            "Epoch 69/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.6335 - mae: 2.6335 - val_loss: 2.7858 - val_mae: 2.7858\n",
            "Epoch 70/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.5235 - mae: 2.5235 - val_loss: 2.6588 - val_mae: 2.6588\n",
            "Epoch 71/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4768 - mae: 2.4768 - val_loss: 2.6746 - val_mae: 2.6746\n",
            "Epoch 72/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 2.4342 - mae: 2.4342 - val_loss: 2.5188 - val_mae: 2.5188\n",
            "Epoch 73/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4324 - mae: 2.4324 - val_loss: 2.7927 - val_mae: 2.7927\n",
            "Epoch 74/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.5069 - mae: 2.5069 - val_loss: 2.6460 - val_mae: 2.6460\n",
            "Epoch 75/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4748 - mae: 2.4748 - val_loss: 2.7015 - val_mae: 2.7015\n",
            "Epoch 76/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.5345 - mae: 2.5345 - val_loss: 2.6579 - val_mae: 2.6579\n",
            "Epoch 77/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.3769 - mae: 2.3769 - val_loss: 2.4283 - val_mae: 2.4283\n",
            "Epoch 78/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4445 - mae: 2.4445 - val_loss: 2.5767 - val_mae: 2.5767\n",
            "Epoch 79/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4465 - mae: 2.4465 - val_loss: 2.5118 - val_mae: 2.5118\n",
            "Epoch 80/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.3260 - mae: 2.3260 - val_loss: 2.6213 - val_mae: 2.6213\n",
            "Epoch 81/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.3523 - mae: 2.3523 - val_loss: 2.1815 - val_mae: 2.1815\n",
            "Epoch 82/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.3967 - mae: 2.3967 - val_loss: 2.5906 - val_mae: 2.5906\n",
            "Epoch 83/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.4382 - mae: 2.4382 - val_loss: 2.3379 - val_mae: 2.3379\n",
            "Epoch 84/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2609 - mae: 2.2609 - val_loss: 2.3273 - val_mae: 2.3273\n",
            "Epoch 85/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2736 - mae: 2.2736 - val_loss: 2.4462 - val_mae: 2.4462\n",
            "Epoch 86/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2929 - mae: 2.2929 - val_loss: 2.4431 - val_mae: 2.4431\n",
            "Epoch 87/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.3099 - mae: 2.3099 - val_loss: 2.4249 - val_mae: 2.4249\n",
            "Epoch 88/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.2538 - mae: 2.2538 - val_loss: 2.4780 - val_mae: 2.4780\n",
            "Epoch 89/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2625 - mae: 2.2625 - val_loss: 2.4159 - val_mae: 2.4159\n",
            "Epoch 90/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 2.2309 - mae: 2.2309 - val_loss: 2.5580 - val_mae: 2.5580\n",
            "Epoch 91/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2546 - mae: 2.2546 - val_loss: 2.3723 - val_mae: 2.3723\n",
            "Epoch 92/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.2324 - mae: 2.2324 - val_loss: 2.2980 - val_mae: 2.2980\n",
            "Epoch 93/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2241 - mae: 2.2241 - val_loss: 2.2750 - val_mae: 2.2750\n",
            "Epoch 94/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.1598 - mae: 2.1598 - val_loss: 2.2862 - val_mae: 2.2862\n",
            "Epoch 95/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.2200 - mae: 2.2200 - val_loss: 2.2142 - val_mae: 2.2142\n",
            "Epoch 96/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.1672 - mae: 2.1672 - val_loss: 2.4507 - val_mae: 2.4507\n",
            "Epoch 97/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.1354 - mae: 2.1354 - val_loss: 2.3054 - val_mae: 2.3054\n",
            "Epoch 98/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 2.1053 - mae: 2.1053 - val_loss: 2.0678 - val_mae: 2.0678\n",
            "Epoch 99/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.1706 - mae: 2.1706 - val_loss: 2.1544 - val_mae: 2.1544\n",
            "Epoch 100/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0826 - mae: 2.0826 - val_loss: 2.2622 - val_mae: 2.2622\n",
            "Epoch 101/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.1869 - mae: 2.1869 - val_loss: 2.3907 - val_mae: 2.3907\n",
            "Epoch 102/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.1252 - mae: 2.1252 - val_loss: 2.2686 - val_mae: 2.2686\n",
            "Epoch 103/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.1081 - mae: 2.1081 - val_loss: 2.1353 - val_mae: 2.1353\n",
            "Epoch 104/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.1855 - mae: 2.1855 - val_loss: 2.0748 - val_mae: 2.0748\n",
            "Epoch 105/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0222 - mae: 2.0222 - val_loss: 1.9497 - val_mae: 1.9497\n",
            "Epoch 106/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 2.0647 - mae: 2.0647 - val_loss: 2.3662 - val_mae: 2.3662\n",
            "Epoch 107/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0610 - mae: 2.0610 - val_loss: 2.1008 - val_mae: 2.1008\n",
            "Epoch 108/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0468 - mae: 2.0468 - val_loss: 2.1184 - val_mae: 2.1184\n",
            "Epoch 109/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.0353 - mae: 2.0353 - val_loss: 1.9125 - val_mae: 1.9125\n",
            "Epoch 110/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.0261 - mae: 2.0261 - val_loss: 1.9826 - val_mae: 1.9826\n",
            "Epoch 111/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.0368 - mae: 2.0368 - val_loss: 2.0576 - val_mae: 2.0576\n",
            "Epoch 112/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0506 - mae: 2.0506 - val_loss: 1.9273 - val_mae: 1.9273\n",
            "Epoch 113/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9982 - mae: 1.9982 - val_loss: 2.0782 - val_mae: 2.0782\n",
            "Epoch 114/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9998 - mae: 1.9998 - val_loss: 2.2009 - val_mae: 2.2009\n",
            "Epoch 115/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9948 - mae: 1.9948 - val_loss: 2.3669 - val_mae: 2.3669\n",
            "Epoch 116/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 2.0708 - mae: 2.0708 - val_loss: 1.9628 - val_mae: 1.9628\n",
            "Epoch 117/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0313 - mae: 2.0313 - val_loss: 1.9008 - val_mae: 1.9008\n",
            "Epoch 118/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9341 - mae: 1.9341 - val_loss: 1.8205 - val_mae: 1.8205\n",
            "Epoch 119/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9333 - mae: 1.9333 - val_loss: 2.0161 - val_mae: 2.0161\n",
            "Epoch 120/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.9966 - mae: 1.9966 - val_loss: 2.1862 - val_mae: 2.1862\n",
            "Epoch 121/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.9463 - mae: 1.9463 - val_loss: 2.1003 - val_mae: 2.1003\n",
            "Epoch 122/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 2.0044 - mae: 2.0044 - val_loss: 2.0685 - val_mae: 2.0685\n",
            "Epoch 123/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0127 - mae: 2.0127 - val_loss: 2.0475 - val_mae: 2.0475\n",
            "Epoch 124/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9577 - mae: 1.9577 - val_loss: 2.0706 - val_mae: 2.0706\n",
            "Epoch 125/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9545 - mae: 1.9545 - val_loss: 1.9720 - val_mae: 1.9720\n",
            "Epoch 126/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9687 - mae: 1.9687 - val_loss: 2.0402 - val_mae: 2.0402\n",
            "Epoch 127/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 2.0122 - mae: 2.0122 - val_loss: 1.8171 - val_mae: 1.8171\n",
            "Epoch 128/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9219 - mae: 1.9219 - val_loss: 1.6783 - val_mae: 1.6783\n",
            "Epoch 129/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8744 - mae: 1.8744 - val_loss: 1.9099 - val_mae: 1.9099\n",
            "Epoch 130/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9820 - mae: 1.9820 - val_loss: 2.1659 - val_mae: 2.1659\n",
            "Epoch 131/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.9282 - mae: 1.9282 - val_loss: 2.3284 - val_mae: 2.3284\n",
            "Epoch 132/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.8951 - mae: 1.8951 - val_loss: 2.2681 - val_mae: 2.2681\n",
            "Epoch 133/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.8775 - mae: 1.8775 - val_loss: 1.9162 - val_mae: 1.9162\n",
            "Epoch 134/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.9014 - mae: 1.9014 - val_loss: 1.8859 - val_mae: 1.8859\n",
            "Epoch 135/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.9340 - mae: 1.9340 - val_loss: 1.6849 - val_mae: 1.6849\n",
            "Epoch 136/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.8824 - mae: 1.8824 - val_loss: 1.8179 - val_mae: 1.8179\n",
            "Epoch 137/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.7886 - mae: 1.7886 - val_loss: 1.7279 - val_mae: 1.7279\n",
            "Epoch 138/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.9598 - mae: 1.9598 - val_loss: 1.8194 - val_mae: 1.8194\n",
            "Epoch 139/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.9120 - mae: 1.9120 - val_loss: 1.8089 - val_mae: 1.8089\n",
            "Epoch 140/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.8698 - mae: 1.8698 - val_loss: 1.8559 - val_mae: 1.8559\n",
            "Epoch 141/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.8748 - mae: 1.8748 - val_loss: 2.1947 - val_mae: 2.1947\n",
            "Epoch 142/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8397 - mae: 1.8397 - val_loss: 1.8703 - val_mae: 1.8703\n",
            "Epoch 143/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8520 - mae: 1.8520 - val_loss: 1.9626 - val_mae: 1.9626\n",
            "Epoch 144/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.9092 - mae: 1.9092 - val_loss: 1.8194 - val_mae: 1.8194\n",
            "Epoch 145/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8629 - mae: 1.8629 - val_loss: 1.8358 - val_mae: 1.8358\n",
            "Epoch 146/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8970 - mae: 1.8970 - val_loss: 1.9735 - val_mae: 1.9735\n",
            "Epoch 147/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8343 - mae: 1.8343 - val_loss: 1.7973 - val_mae: 1.7973\n",
            "Epoch 148/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8381 - mae: 1.8381 - val_loss: 1.8858 - val_mae: 1.8858\n",
            "Epoch 149/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8041 - mae: 1.8041 - val_loss: 2.0652 - val_mae: 2.0652\n",
            "Epoch 150/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.9488 - mae: 1.9488 - val_loss: 1.7218 - val_mae: 1.7218\n",
            "Epoch 151/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7587 - mae: 1.7587 - val_loss: 1.8773 - val_mae: 1.8773\n",
            "Epoch 152/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7943 - mae: 1.7943 - val_loss: 1.9070 - val_mae: 1.9070\n",
            "Epoch 153/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7501 - mae: 1.7501 - val_loss: 1.9399 - val_mae: 1.9399\n",
            "Epoch 154/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8084 - mae: 1.8084 - val_loss: 1.7963 - val_mae: 1.7963\n",
            "Epoch 155/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8300 - mae: 1.8300 - val_loss: 1.6753 - val_mae: 1.6753\n",
            "Epoch 156/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.8318 - mae: 1.8318 - val_loss: 1.7284 - val_mae: 1.7284\n",
            "Epoch 157/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7258 - mae: 1.7258 - val_loss: 1.8283 - val_mae: 1.8283\n",
            "Epoch 158/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7498 - mae: 1.7498 - val_loss: 1.7624 - val_mae: 1.7624\n",
            "Epoch 159/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.7498 - mae: 1.7498 - val_loss: 1.7920 - val_mae: 1.7920\n",
            "Epoch 160/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8261 - mae: 1.8261 - val_loss: 2.0320 - val_mae: 2.0320\n",
            "Epoch 161/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7858 - mae: 1.7858 - val_loss: 1.5901 - val_mae: 1.5901\n",
            "Epoch 162/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7784 - mae: 1.7784 - val_loss: 1.7651 - val_mae: 1.7651\n",
            "Epoch 163/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.8101 - mae: 1.8101 - val_loss: 1.7134 - val_mae: 1.7134\n",
            "Epoch 164/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.8153 - mae: 1.8153 - val_loss: 1.7264 - val_mae: 1.7264\n",
            "Epoch 165/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.7109 - mae: 1.7109 - val_loss: 1.6885 - val_mae: 1.6885\n",
            "Epoch 166/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6389 - mae: 1.6389 - val_loss: 1.9624 - val_mae: 1.9624\n",
            "Epoch 167/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7354 - mae: 1.7354 - val_loss: 1.6247 - val_mae: 1.6247\n",
            "Epoch 168/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.7156 - mae: 1.7156 - val_loss: 1.7718 - val_mae: 1.7718\n",
            "Epoch 169/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7653 - mae: 1.7653 - val_loss: 1.6525 - val_mae: 1.6525\n",
            "Epoch 170/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6931 - mae: 1.6931 - val_loss: 1.5572 - val_mae: 1.5572\n",
            "Epoch 171/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.7223 - mae: 1.7223 - val_loss: 1.9859 - val_mae: 1.9859\n",
            "Epoch 172/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.8507 - mae: 1.8507 - val_loss: 1.5442 - val_mae: 1.5442\n",
            "Epoch 173/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6512 - mae: 1.6512 - val_loss: 1.6621 - val_mae: 1.6621\n",
            "Epoch 174/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6814 - mae: 1.6814 - val_loss: 1.6413 - val_mae: 1.6413\n",
            "Epoch 175/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7417 - mae: 1.7417 - val_loss: 1.8700 - val_mae: 1.8700\n",
            "Epoch 176/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7565 - mae: 1.7565 - val_loss: 1.7259 - val_mae: 1.7259\n",
            "Epoch 177/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6861 - mae: 1.6861 - val_loss: 1.7177 - val_mae: 1.7177\n",
            "Epoch 178/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7313 - mae: 1.7313 - val_loss: 1.8245 - val_mae: 1.8245\n",
            "Epoch 179/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6784 - mae: 1.6784 - val_loss: 1.6241 - val_mae: 1.6241\n",
            "Epoch 180/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7047 - mae: 1.7047 - val_loss: 1.5052 - val_mae: 1.5052\n",
            "Epoch 181/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6532 - mae: 1.6532 - val_loss: 1.7487 - val_mae: 1.7487\n",
            "Epoch 182/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6948 - mae: 1.6948 - val_loss: 1.7375 - val_mae: 1.7375\n",
            "Epoch 183/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7199 - mae: 1.7199 - val_loss: 1.6467 - val_mae: 1.6467\n",
            "Epoch 184/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6722 - mae: 1.6722 - val_loss: 1.4769 - val_mae: 1.4769\n",
            "Epoch 185/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6841 - mae: 1.6841 - val_loss: 1.8064 - val_mae: 1.8064\n",
            "Epoch 186/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.7461 - mae: 1.7461 - val_loss: 1.5150 - val_mae: 1.5150\n",
            "Epoch 187/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6336 - mae: 1.6336 - val_loss: 1.6483 - val_mae: 1.6483\n",
            "Epoch 188/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6700 - mae: 1.6700 - val_loss: 1.6015 - val_mae: 1.6015\n",
            "Epoch 189/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6675 - mae: 1.6675 - val_loss: 1.6265 - val_mae: 1.6265\n",
            "Epoch 190/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6863 - mae: 1.6863 - val_loss: 1.4752 - val_mae: 1.4752\n",
            "Epoch 191/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5833 - mae: 1.5833 - val_loss: 1.4937 - val_mae: 1.4937\n",
            "Epoch 192/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6551 - mae: 1.6551 - val_loss: 1.6890 - val_mae: 1.6890\n",
            "Epoch 193/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6624 - mae: 1.6624 - val_loss: 1.6943 - val_mae: 1.6943\n",
            "Epoch 194/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6390 - mae: 1.6390 - val_loss: 1.5581 - val_mae: 1.5581\n",
            "Epoch 195/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6834 - mae: 1.6834 - val_loss: 1.5006 - val_mae: 1.5006\n",
            "Epoch 196/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6279 - mae: 1.6279 - val_loss: 1.5882 - val_mae: 1.5882\n",
            "Epoch 197/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6611 - mae: 1.6611 - val_loss: 1.5622 - val_mae: 1.5622\n",
            "Epoch 198/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5963 - mae: 1.5963 - val_loss: 1.5684 - val_mae: 1.5684\n",
            "Epoch 199/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6451 - mae: 1.6451 - val_loss: 1.4420 - val_mae: 1.4420\n",
            "Epoch 200/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6896 - mae: 1.6896 - val_loss: 1.4502 - val_mae: 1.4502\n",
            "Epoch 201/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6050 - mae: 1.6050 - val_loss: 1.4743 - val_mae: 1.4743\n",
            "Epoch 202/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6563 - mae: 1.6563 - val_loss: 1.6331 - val_mae: 1.6331\n",
            "Epoch 203/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6391 - mae: 1.6391 - val_loss: 1.3727 - val_mae: 1.3727\n",
            "Epoch 204/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5881 - mae: 1.5881 - val_loss: 1.6678 - val_mae: 1.6678\n",
            "Epoch 205/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6257 - mae: 1.6257 - val_loss: 1.5054 - val_mae: 1.5054\n",
            "Epoch 206/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5528 - mae: 1.5528 - val_loss: 1.4791 - val_mae: 1.4791\n",
            "Epoch 207/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.7072 - mae: 1.7072 - val_loss: 1.6412 - val_mae: 1.6412\n",
            "Epoch 208/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6205 - mae: 1.6205 - val_loss: 1.5744 - val_mae: 1.5744\n",
            "Epoch 209/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5839 - mae: 1.5839 - val_loss: 1.6702 - val_mae: 1.6702\n",
            "Epoch 210/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.6030 - mae: 1.6030 - val_loss: 1.5490 - val_mae: 1.5490\n",
            "Epoch 211/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5788 - mae: 1.5788 - val_loss: 1.5107 - val_mae: 1.5107\n",
            "Epoch 212/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5890 - mae: 1.5890 - val_loss: 1.8202 - val_mae: 1.8202\n",
            "Epoch 213/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6107 - mae: 1.6107 - val_loss: 1.5161 - val_mae: 1.5161\n",
            "Epoch 214/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6176 - mae: 1.6176 - val_loss: 1.5340 - val_mae: 1.5340\n",
            "Epoch 215/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5913 - mae: 1.5913 - val_loss: 1.5099 - val_mae: 1.5099\n",
            "Epoch 216/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6492 - mae: 1.6492 - val_loss: 1.4478 - val_mae: 1.4478\n",
            "Epoch 217/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6340 - mae: 1.6340 - val_loss: 1.4776 - val_mae: 1.4776\n",
            "Epoch 218/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5974 - mae: 1.5974 - val_loss: 1.6490 - val_mae: 1.6490\n",
            "Epoch 219/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5450 - mae: 1.5450 - val_loss: 1.3283 - val_mae: 1.3283\n",
            "Epoch 220/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5563 - mae: 1.5563 - val_loss: 1.4527 - val_mae: 1.4527\n",
            "Epoch 221/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5225 - mae: 1.5225 - val_loss: 1.3991 - val_mae: 1.3991\n",
            "Epoch 222/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5786 - mae: 1.5786 - val_loss: 1.5764 - val_mae: 1.5764\n",
            "Epoch 223/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5893 - mae: 1.5893 - val_loss: 1.3032 - val_mae: 1.3032\n",
            "Epoch 224/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5326 - mae: 1.5326 - val_loss: 1.4052 - val_mae: 1.4052\n",
            "Epoch 225/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5389 - mae: 1.5389 - val_loss: 1.4780 - val_mae: 1.4780\n",
            "Epoch 226/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5596 - mae: 1.5596 - val_loss: 1.4895 - val_mae: 1.4895\n",
            "Epoch 227/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6231 - mae: 1.6231 - val_loss: 1.3674 - val_mae: 1.3674\n",
            "Epoch 228/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5808 - mae: 1.5808 - val_loss: 1.5101 - val_mae: 1.5101\n",
            "Epoch 229/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6217 - mae: 1.6217 - val_loss: 1.4385 - val_mae: 1.4385\n",
            "Epoch 230/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6001 - mae: 1.6001 - val_loss: 1.4500 - val_mae: 1.4500\n",
            "Epoch 231/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5221 - mae: 1.5221 - val_loss: 1.6664 - val_mae: 1.6664\n",
            "Epoch 232/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6265 - mae: 1.6265 - val_loss: 1.5248 - val_mae: 1.5248\n",
            "Epoch 233/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5553 - mae: 1.5553 - val_loss: 1.4355 - val_mae: 1.4355\n",
            "Epoch 234/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5296 - mae: 1.5296 - val_loss: 1.5106 - val_mae: 1.5106\n",
            "Epoch 235/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5262 - mae: 1.5262 - val_loss: 1.5441 - val_mae: 1.5441\n",
            "Epoch 236/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4983 - mae: 1.4983 - val_loss: 1.8156 - val_mae: 1.8156\n",
            "Epoch 237/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.6247 - mae: 1.6247 - val_loss: 1.4877 - val_mae: 1.4877\n",
            "Epoch 238/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.6105 - mae: 1.6105 - val_loss: 1.5438 - val_mae: 1.5438\n",
            "Epoch 239/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5612 - mae: 1.5612 - val_loss: 1.4707 - val_mae: 1.4707\n",
            "Epoch 240/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5674 - mae: 1.5674 - val_loss: 1.4303 - val_mae: 1.4303\n",
            "Epoch 241/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5576 - mae: 1.5576 - val_loss: 1.3918 - val_mae: 1.3918\n",
            "Epoch 242/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5998 - mae: 1.5998 - val_loss: 1.4963 - val_mae: 1.4963\n",
            "Epoch 243/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5715 - mae: 1.5715 - val_loss: 1.4309 - val_mae: 1.4309\n",
            "Epoch 244/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5575 - mae: 1.5575 - val_loss: 1.4879 - val_mae: 1.4879\n",
            "Epoch 245/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5397 - mae: 1.5397 - val_loss: 1.4456 - val_mae: 1.4456\n",
            "Epoch 246/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.5450 - mae: 1.5450 - val_loss: 1.4165 - val_mae: 1.4165\n",
            "Epoch 247/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5903 - mae: 1.5903 - val_loss: 1.3291 - val_mae: 1.3291\n",
            "Epoch 248/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5246 - mae: 1.5246 - val_loss: 1.4158 - val_mae: 1.4158\n",
            "Epoch 249/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5351 - mae: 1.5351 - val_loss: 1.2578 - val_mae: 1.2578\n",
            "Epoch 250/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5277 - mae: 1.5277 - val_loss: 1.4203 - val_mae: 1.4203\n",
            "Epoch 251/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5382 - mae: 1.5382 - val_loss: 1.3632 - val_mae: 1.3632\n",
            "Epoch 252/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5978 - mae: 1.5978 - val_loss: 1.2432 - val_mae: 1.2432\n",
            "Epoch 253/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5342 - mae: 1.5342 - val_loss: 1.2888 - val_mae: 1.2888\n",
            "Epoch 254/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5219 - mae: 1.5219 - val_loss: 1.5107 - val_mae: 1.5107\n",
            "Epoch 255/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4401 - mae: 1.4401 - val_loss: 1.2969 - val_mae: 1.2969\n",
            "Epoch 256/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4750 - mae: 1.4750 - val_loss: 1.3116 - val_mae: 1.3116\n",
            "Epoch 257/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4896 - mae: 1.4896 - val_loss: 1.2727 - val_mae: 1.2727\n",
            "Epoch 258/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4515 - mae: 1.4515 - val_loss: 1.4760 - val_mae: 1.4760\n",
            "Epoch 259/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5546 - mae: 1.5546 - val_loss: 1.4745 - val_mae: 1.4745\n",
            "Epoch 260/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4663 - mae: 1.4663 - val_loss: 1.4866 - val_mae: 1.4866\n",
            "Epoch 261/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5471 - mae: 1.5471 - val_loss: 1.4289 - val_mae: 1.4289\n",
            "Epoch 262/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5325 - mae: 1.5325 - val_loss: 1.4077 - val_mae: 1.4077\n",
            "Epoch 263/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4490 - mae: 1.4490 - val_loss: 1.3037 - val_mae: 1.3037\n",
            "Epoch 264/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4441 - mae: 1.4441 - val_loss: 1.3545 - val_mae: 1.3545\n",
            "Epoch 265/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5299 - mae: 1.5299 - val_loss: 1.3975 - val_mae: 1.3975\n",
            "Epoch 266/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5534 - mae: 1.5534 - val_loss: 1.3853 - val_mae: 1.3853\n",
            "Epoch 267/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5405 - mae: 1.5405 - val_loss: 1.5894 - val_mae: 1.5894\n",
            "Epoch 268/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5644 - mae: 1.5644 - val_loss: 1.5756 - val_mae: 1.5756\n",
            "Epoch 269/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4732 - mae: 1.4732 - val_loss: 1.7060 - val_mae: 1.7060\n",
            "Epoch 270/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5112 - mae: 1.5112 - val_loss: 1.3567 - val_mae: 1.3567\n",
            "Epoch 271/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5064 - mae: 1.5064 - val_loss: 1.3482 - val_mae: 1.3482\n",
            "Epoch 272/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5078 - mae: 1.5078 - val_loss: 1.3674 - val_mae: 1.3674\n",
            "Epoch 273/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5009 - mae: 1.5009 - val_loss: 1.3411 - val_mae: 1.3411\n",
            "Epoch 274/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5135 - mae: 1.5135 - val_loss: 1.6285 - val_mae: 1.6285\n",
            "Epoch 275/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4535 - mae: 1.4535 - val_loss: 1.3033 - val_mae: 1.3033\n",
            "Epoch 276/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4100 - mae: 1.4100 - val_loss: 1.6027 - val_mae: 1.6027\n",
            "Epoch 277/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4424 - mae: 1.4424 - val_loss: 1.3027 - val_mae: 1.3027\n",
            "Epoch 278/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5401 - mae: 1.5401 - val_loss: 1.3006 - val_mae: 1.3006\n",
            "Epoch 279/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.5030 - mae: 1.5030 - val_loss: 1.2818 - val_mae: 1.2818\n",
            "Epoch 280/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4437 - mae: 1.4437 - val_loss: 1.5300 - val_mae: 1.5300\n",
            "Epoch 281/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5002 - mae: 1.5002 - val_loss: 1.4731 - val_mae: 1.4731\n",
            "Epoch 282/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4490 - mae: 1.4490 - val_loss: 1.2081 - val_mae: 1.2081\n",
            "Epoch 283/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.5479 - mae: 1.5479 - val_loss: 1.3526 - val_mae: 1.3526\n",
            "Epoch 284/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3832 - mae: 1.3832 - val_loss: 1.2185 - val_mae: 1.2185\n",
            "Epoch 285/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4164 - mae: 1.4164 - val_loss: 1.2864 - val_mae: 1.2864\n",
            "Epoch 286/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4028 - mae: 1.4028 - val_loss: 1.2083 - val_mae: 1.2083\n",
            "Epoch 287/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4282 - mae: 1.4282 - val_loss: 1.2853 - val_mae: 1.2853\n",
            "Epoch 288/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4837 - mae: 1.4837 - val_loss: 1.2879 - val_mae: 1.2879\n",
            "Epoch 289/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4408 - mae: 1.4408 - val_loss: 1.2535 - val_mae: 1.2535\n",
            "Epoch 290/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4660 - mae: 1.4660 - val_loss: 1.3373 - val_mae: 1.3373\n",
            "Epoch 291/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3830 - mae: 1.3830 - val_loss: 1.2997 - val_mae: 1.2997\n",
            "Epoch 292/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4137 - mae: 1.4137 - val_loss: 1.2350 - val_mae: 1.2350\n",
            "Epoch 293/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4345 - mae: 1.4345 - val_loss: 1.2354 - val_mae: 1.2354\n",
            "Epoch 294/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3830 - mae: 1.3830 - val_loss: 1.1881 - val_mae: 1.1881\n",
            "Epoch 295/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3969 - mae: 1.3969 - val_loss: 1.2967 - val_mae: 1.2967\n",
            "Epoch 296/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3976 - mae: 1.3976 - val_loss: 1.3181 - val_mae: 1.3181\n",
            "Epoch 297/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4371 - mae: 1.4371 - val_loss: 1.5401 - val_mae: 1.5401\n",
            "Epoch 298/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4396 - mae: 1.4396 - val_loss: 1.2856 - val_mae: 1.2856\n",
            "Epoch 299/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4276 - mae: 1.4276 - val_loss: 1.3229 - val_mae: 1.3229\n",
            "Epoch 300/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4279 - mae: 1.4279 - val_loss: 1.4376 - val_mae: 1.4376\n",
            "Epoch 301/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4689 - mae: 1.4689 - val_loss: 1.2341 - val_mae: 1.2341\n",
            "Epoch 302/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4531 - mae: 1.4531 - val_loss: 1.3721 - val_mae: 1.3721\n",
            "Epoch 303/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4260 - mae: 1.4260 - val_loss: 1.3424 - val_mae: 1.3424\n",
            "Epoch 304/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4479 - mae: 1.4479 - val_loss: 1.3991 - val_mae: 1.3991\n",
            "Epoch 305/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3707 - mae: 1.3707 - val_loss: 1.2784 - val_mae: 1.2784\n",
            "Epoch 306/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3966 - mae: 1.3966 - val_loss: 1.3258 - val_mae: 1.3258\n",
            "Epoch 307/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4544 - mae: 1.4544 - val_loss: 1.3237 - val_mae: 1.3237\n",
            "Epoch 308/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4574 - mae: 1.4574 - val_loss: 1.3345 - val_mae: 1.3345\n",
            "Epoch 309/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3935 - mae: 1.3935 - val_loss: 1.2515 - val_mae: 1.2515\n",
            "Epoch 310/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3231 - mae: 1.3231 - val_loss: 1.2494 - val_mae: 1.2494\n",
            "Epoch 311/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3850 - mae: 1.3850 - val_loss: 1.3894 - val_mae: 1.3894\n",
            "Epoch 312/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3628 - mae: 1.3628 - val_loss: 1.2322 - val_mae: 1.2322\n",
            "Epoch 313/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4222 - mae: 1.4222 - val_loss: 1.3309 - val_mae: 1.3309\n",
            "Epoch 314/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3643 - mae: 1.3643 - val_loss: 1.5160 - val_mae: 1.5160\n",
            "Epoch 315/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4205 - mae: 1.4205 - val_loss: 1.1659 - val_mae: 1.1659\n",
            "Epoch 316/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3685 - mae: 1.3685 - val_loss: 1.5669 - val_mae: 1.5669\n",
            "Epoch 317/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3559 - mae: 1.3559 - val_loss: 1.2882 - val_mae: 1.2882\n",
            "Epoch 318/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3419 - mae: 1.3419 - val_loss: 1.2506 - val_mae: 1.2506\n",
            "Epoch 319/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4286 - mae: 1.4286 - val_loss: 1.1304 - val_mae: 1.1304\n",
            "Epoch 320/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3995 - mae: 1.3995 - val_loss: 1.1433 - val_mae: 1.1433\n",
            "Epoch 321/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4379 - mae: 1.4379 - val_loss: 1.2187 - val_mae: 1.2187\n",
            "Epoch 322/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3466 - mae: 1.3466 - val_loss: 1.2214 - val_mae: 1.2214\n",
            "Epoch 323/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4197 - mae: 1.4197 - val_loss: 1.0836 - val_mae: 1.0836\n",
            "Epoch 324/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3827 - mae: 1.3827 - val_loss: 1.2510 - val_mae: 1.2510\n",
            "Epoch 325/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3471 - mae: 1.3471 - val_loss: 1.2123 - val_mae: 1.2123\n",
            "Epoch 326/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4139 - mae: 1.4139 - val_loss: 1.4513 - val_mae: 1.4513\n",
            "Epoch 327/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3443 - mae: 1.3443 - val_loss: 1.2541 - val_mae: 1.2541\n",
            "Epoch 328/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4065 - mae: 1.4065 - val_loss: 1.3210 - val_mae: 1.3210\n",
            "Epoch 329/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2797 - mae: 1.2797 - val_loss: 1.1998 - val_mae: 1.1998\n",
            "Epoch 330/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4059 - mae: 1.4059 - val_loss: 1.2034 - val_mae: 1.2034\n",
            "Epoch 331/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3783 - mae: 1.3783 - val_loss: 1.0802 - val_mae: 1.0802\n",
            "Epoch 332/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3431 - mae: 1.3431 - val_loss: 1.2089 - val_mae: 1.2089\n",
            "Epoch 333/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4334 - mae: 1.4334 - val_loss: 1.1880 - val_mae: 1.1880\n",
            "Epoch 334/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3841 - mae: 1.3841 - val_loss: 1.0825 - val_mae: 1.0825\n",
            "Epoch 335/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3601 - mae: 1.3601 - val_loss: 1.1430 - val_mae: 1.1430\n",
            "Epoch 336/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3618 - mae: 1.3618 - val_loss: 1.1711 - val_mae: 1.1711\n",
            "Epoch 337/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3168 - mae: 1.3168 - val_loss: 1.1503 - val_mae: 1.1503\n",
            "Epoch 338/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4125 - mae: 1.4125 - val_loss: 1.3474 - val_mae: 1.3474\n",
            "Epoch 339/1000\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 1.3674 - mae: 1.3674 - val_loss: 1.3001 - val_mae: 1.3001\n",
            "Epoch 340/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3606 - mae: 1.3606 - val_loss: 1.5194 - val_mae: 1.5194\n",
            "Epoch 341/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4346 - mae: 1.4346 - val_loss: 1.4632 - val_mae: 1.4632\n",
            "Epoch 342/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3545 - mae: 1.3545 - val_loss: 1.4566 - val_mae: 1.4566\n",
            "Epoch 343/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4782 - mae: 1.4782 - val_loss: 1.1800 - val_mae: 1.1800\n",
            "Epoch 344/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4117 - mae: 1.4117 - val_loss: 1.4640 - val_mae: 1.4640\n",
            "Epoch 345/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3369 - mae: 1.3369 - val_loss: 1.3398 - val_mae: 1.3398\n",
            "Epoch 346/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3551 - mae: 1.3551 - val_loss: 1.2290 - val_mae: 1.2290\n",
            "Epoch 347/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3761 - mae: 1.3761 - val_loss: 1.2180 - val_mae: 1.2180\n",
            "Epoch 348/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3883 - mae: 1.3883 - val_loss: 1.0714 - val_mae: 1.0714\n",
            "Epoch 349/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3955 - mae: 1.3955 - val_loss: 1.1796 - val_mae: 1.1796\n",
            "Epoch 350/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3570 - mae: 1.3570 - val_loss: 1.1872 - val_mae: 1.1872\n",
            "Epoch 351/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3592 - mae: 1.3592 - val_loss: 1.1425 - val_mae: 1.1425\n",
            "Epoch 352/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4017 - mae: 1.4017 - val_loss: 1.2811 - val_mae: 1.2811\n",
            "Epoch 353/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3163 - mae: 1.3163 - val_loss: 1.2298 - val_mae: 1.2298\n",
            "Epoch 354/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3561 - mae: 1.3561 - val_loss: 1.1601 - val_mae: 1.1601\n",
            "Epoch 355/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.1011 - val_mae: 1.1011\n",
            "Epoch 356/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3803 - mae: 1.3803 - val_loss: 1.3164 - val_mae: 1.3164\n",
            "Epoch 357/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3674 - mae: 1.3674 - val_loss: 1.2366 - val_mae: 1.2366\n",
            "Epoch 358/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3295 - mae: 1.3295 - val_loss: 1.1982 - val_mae: 1.1982\n",
            "Epoch 359/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2888 - mae: 1.2888 - val_loss: 1.1692 - val_mae: 1.1692\n",
            "Epoch 360/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2846 - mae: 1.2846 - val_loss: 1.4236 - val_mae: 1.4236\n",
            "Epoch 361/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3562 - mae: 1.3562 - val_loss: 1.1704 - val_mae: 1.1704\n",
            "Epoch 362/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3714 - mae: 1.3714 - val_loss: 1.2787 - val_mae: 1.2787\n",
            "Epoch 363/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3090 - mae: 1.3090 - val_loss: 1.2395 - val_mae: 1.2395\n",
            "Epoch 364/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3019 - mae: 1.3019 - val_loss: 1.1853 - val_mae: 1.1853\n",
            "Epoch 365/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.4081 - mae: 1.4081 - val_loss: 1.2054 - val_mae: 1.2054\n",
            "Epoch 366/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3435 - mae: 1.3435 - val_loss: 1.1680 - val_mae: 1.1680\n",
            "Epoch 367/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3746 - mae: 1.3746 - val_loss: 1.2084 - val_mae: 1.2084\n",
            "Epoch 368/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3205 - mae: 1.3205 - val_loss: 1.1938 - val_mae: 1.1938\n",
            "Epoch 369/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.4566 - mae: 1.4566 - val_loss: 1.2360 - val_mae: 1.2360\n",
            "Epoch 370/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2888 - mae: 1.2888 - val_loss: 1.3980 - val_mae: 1.3980\n",
            "Epoch 371/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3113 - mae: 1.3113 - val_loss: 1.1465 - val_mae: 1.1465\n",
            "Epoch 372/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3457 - mae: 1.3457 - val_loss: 1.2488 - val_mae: 1.2488\n",
            "Epoch 373/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2781 - mae: 1.2781 - val_loss: 1.1388 - val_mae: 1.1388\n",
            "Epoch 374/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3099 - mae: 1.3099 - val_loss: 1.1490 - val_mae: 1.1490\n",
            "Epoch 375/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3196 - mae: 1.3196 - val_loss: 1.1883 - val_mae: 1.1883\n",
            "Epoch 376/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2896 - mae: 1.2896 - val_loss: 1.2125 - val_mae: 1.2125\n",
            "Epoch 377/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3040 - mae: 1.3040 - val_loss: 1.2454 - val_mae: 1.2454\n",
            "Epoch 378/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3278 - mae: 1.3278 - val_loss: 1.1445 - val_mae: 1.1445\n",
            "Epoch 379/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3812 - mae: 1.3812 - val_loss: 1.0345 - val_mae: 1.0345\n",
            "Epoch 380/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2827 - mae: 1.2827 - val_loss: 1.2039 - val_mae: 1.2039\n",
            "Epoch 381/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3525 - mae: 1.3525 - val_loss: 1.2780 - val_mae: 1.2780\n",
            "Epoch 382/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2981 - mae: 1.2981 - val_loss: 1.0421 - val_mae: 1.0421\n",
            "Epoch 383/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3442 - mae: 1.3442 - val_loss: 1.3366 - val_mae: 1.3366\n",
            "Epoch 384/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3242 - mae: 1.3242 - val_loss: 1.2335 - val_mae: 1.2335\n",
            "Epoch 385/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2241 - mae: 1.2241 - val_loss: 1.1629 - val_mae: 1.1629\n",
            "Epoch 386/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2848 - mae: 1.2848 - val_loss: 1.1605 - val_mae: 1.1605\n",
            "Epoch 387/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2758 - mae: 1.2758 - val_loss: 1.1556 - val_mae: 1.1556\n",
            "Epoch 388/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3282 - mae: 1.3282 - val_loss: 1.2074 - val_mae: 1.2074\n",
            "Epoch 389/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2733 - mae: 1.2733 - val_loss: 1.3597 - val_mae: 1.3597\n",
            "Epoch 390/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3843 - mae: 1.3843 - val_loss: 1.0941 - val_mae: 1.0941\n",
            "Epoch 391/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2338 - mae: 1.2338 - val_loss: 1.1045 - val_mae: 1.1045\n",
            "Epoch 392/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2518 - mae: 1.2518 - val_loss: 1.2438 - val_mae: 1.2438\n",
            "Epoch 393/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2654 - mae: 1.2654 - val_loss: 1.0768 - val_mae: 1.0768\n",
            "Epoch 394/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3037 - mae: 1.3037 - val_loss: 1.1157 - val_mae: 1.1157\n",
            "Epoch 395/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2840 - mae: 1.2840 - val_loss: 1.1606 - val_mae: 1.1606\n",
            "Epoch 396/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2854 - mae: 1.2854 - val_loss: 1.2499 - val_mae: 1.2499\n",
            "Epoch 397/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2858 - mae: 1.2858 - val_loss: 1.0693 - val_mae: 1.0693\n",
            "Epoch 398/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2406 - mae: 1.2406 - val_loss: 1.1753 - val_mae: 1.1753\n",
            "Epoch 399/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2780 - mae: 1.2780 - val_loss: 1.2213 - val_mae: 1.2213\n",
            "Epoch 400/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3067 - mae: 1.3067 - val_loss: 1.2475 - val_mae: 1.2475\n",
            "Epoch 401/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2534 - mae: 1.2534 - val_loss: 1.1318 - val_mae: 1.1318\n",
            "Epoch 402/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3315 - mae: 1.3315 - val_loss: 1.1320 - val_mae: 1.1320\n",
            "Epoch 403/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3293 - mae: 1.3293 - val_loss: 1.2218 - val_mae: 1.2218\n",
            "Epoch 404/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2467 - mae: 1.2467 - val_loss: 1.3158 - val_mae: 1.3158\n",
            "Epoch 405/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2853 - mae: 1.2853 - val_loss: 1.1020 - val_mae: 1.1020\n",
            "Epoch 406/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2811 - mae: 1.2811 - val_loss: 1.1403 - val_mae: 1.1403\n",
            "Epoch 407/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2386 - mae: 1.2386 - val_loss: 1.0271 - val_mae: 1.0271\n",
            "Epoch 408/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2511 - mae: 1.2511 - val_loss: 1.0588 - val_mae: 1.0588\n",
            "Epoch 409/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2716 - mae: 1.2716 - val_loss: 1.1534 - val_mae: 1.1534\n",
            "Epoch 410/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3465 - mae: 1.3465 - val_loss: 1.1593 - val_mae: 1.1593\n",
            "Epoch 411/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3084 - mae: 1.3084 - val_loss: 1.1096 - val_mae: 1.1096\n",
            "Epoch 412/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2825 - mae: 1.2825 - val_loss: 1.0835 - val_mae: 1.0835\n",
            "Epoch 413/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3030 - mae: 1.3030 - val_loss: 1.2387 - val_mae: 1.2387\n",
            "Epoch 414/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2567 - mae: 1.2567 - val_loss: 1.2267 - val_mae: 1.2267\n",
            "Epoch 415/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2818 - mae: 1.2818 - val_loss: 1.1600 - val_mae: 1.1600\n",
            "Epoch 416/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2236 - mae: 1.2236 - val_loss: 1.1135 - val_mae: 1.1135\n",
            "Epoch 417/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2647 - mae: 1.2647 - val_loss: 1.1193 - val_mae: 1.1193\n",
            "Epoch 418/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2983 - mae: 1.2983 - val_loss: 1.2264 - val_mae: 1.2264\n",
            "Epoch 419/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2022 - mae: 1.2022 - val_loss: 1.1884 - val_mae: 1.1884\n",
            "Epoch 420/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3605 - mae: 1.3605 - val_loss: 1.0269 - val_mae: 1.0269\n",
            "Epoch 421/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2238 - mae: 1.2238 - val_loss: 1.1826 - val_mae: 1.1826\n",
            "Epoch 422/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2202 - mae: 1.2202 - val_loss: 1.0883 - val_mae: 1.0883\n",
            "Epoch 423/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.3279 - mae: 1.3279 - val_loss: 1.2521 - val_mae: 1.2521\n",
            "Epoch 424/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2953 - mae: 1.2953 - val_loss: 1.2198 - val_mae: 1.2198\n",
            "Epoch 425/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3115 - mae: 1.3115 - val_loss: 1.0623 - val_mae: 1.0623\n",
            "Epoch 426/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2042 - mae: 1.2042 - val_loss: 1.1454 - val_mae: 1.1454\n",
            "Epoch 427/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2108 - mae: 1.2108 - val_loss: 1.1588 - val_mae: 1.1588\n",
            "Epoch 428/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2380 - mae: 1.2380 - val_loss: 1.2742 - val_mae: 1.2742\n",
            "Epoch 429/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2503 - mae: 1.2503 - val_loss: 1.0706 - val_mae: 1.0706\n",
            "Epoch 430/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2447 - mae: 1.2447 - val_loss: 1.0793 - val_mae: 1.0793\n",
            "Epoch 431/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2874 - mae: 1.2874 - val_loss: 1.0697 - val_mae: 1.0697\n",
            "Epoch 432/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2830 - mae: 1.2830 - val_loss: 1.1493 - val_mae: 1.1493\n",
            "Epoch 433/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.3014 - mae: 1.3014 - val_loss: 1.1264 - val_mae: 1.1264\n",
            "Epoch 434/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2819 - mae: 1.2819 - val_loss: 1.0708 - val_mae: 1.0708\n",
            "Epoch 435/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2950 - mae: 1.2950 - val_loss: 1.1855 - val_mae: 1.1855\n",
            "Epoch 436/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2243 - mae: 1.2243 - val_loss: 1.0448 - val_mae: 1.0448\n",
            "Epoch 437/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2466 - mae: 1.2466 - val_loss: 1.1114 - val_mae: 1.1114\n",
            "Epoch 438/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2503 - mae: 1.2503 - val_loss: 1.1750 - val_mae: 1.1750\n",
            "Epoch 439/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.2387 - mae: 1.2387 - val_loss: 1.0842 - val_mae: 1.0842\n",
            "Epoch 440/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1896 - mae: 1.1896 - val_loss: 1.0026 - val_mae: 1.0026\n",
            "Epoch 441/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2059 - mae: 1.2059 - val_loss: 1.1315 - val_mae: 1.1315\n",
            "Epoch 442/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2105 - mae: 1.2105 - val_loss: 1.1174 - val_mae: 1.1174\n",
            "Epoch 443/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2058 - mae: 1.2058 - val_loss: 1.0880 - val_mae: 1.0880\n",
            "Epoch 444/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2508 - mae: 1.2508 - val_loss: 1.1178 - val_mae: 1.1178\n",
            "Epoch 445/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2160 - mae: 1.2160 - val_loss: 1.0738 - val_mae: 1.0738\n",
            "Epoch 446/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2313 - mae: 1.2313 - val_loss: 1.1145 - val_mae: 1.1145\n",
            "Epoch 447/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1961 - mae: 1.1961 - val_loss: 1.0655 - val_mae: 1.0655\n",
            "Epoch 448/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2418 - mae: 1.2418 - val_loss: 1.0123 - val_mae: 1.0123\n",
            "Epoch 449/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2368 - mae: 1.2368 - val_loss: 1.4764 - val_mae: 1.4764\n",
            "Epoch 450/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1689 - mae: 1.1689 - val_loss: 1.0139 - val_mae: 1.0139\n",
            "Epoch 451/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1421 - mae: 1.1421 - val_loss: 1.0832 - val_mae: 1.0832\n",
            "Epoch 452/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2945 - mae: 1.2945 - val_loss: 1.0156 - val_mae: 1.0156\n",
            "Epoch 453/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2514 - mae: 1.2514 - val_loss: 1.0010 - val_mae: 1.0010\n",
            "Epoch 454/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2373 - mae: 1.2373 - val_loss: 1.3365 - val_mae: 1.3365\n",
            "Epoch 455/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2224 - mae: 1.2224 - val_loss: 1.1256 - val_mae: 1.1256\n",
            "Epoch 456/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.2694 - mae: 1.2694 - val_loss: 1.1042 - val_mae: 1.1042\n",
            "Epoch 457/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1329 - mae: 1.1329 - val_loss: 1.2297 - val_mae: 1.2297\n",
            "Epoch 458/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2706 - mae: 1.2706 - val_loss: 1.1608 - val_mae: 1.1608\n",
            "Epoch 459/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1568 - mae: 1.1568 - val_loss: 1.0938 - val_mae: 1.0938\n",
            "Epoch 460/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2494 - mae: 1.2494 - val_loss: 1.0858 - val_mae: 1.0858\n",
            "Epoch 461/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1986 - mae: 1.1986 - val_loss: 1.2545 - val_mae: 1.2545\n",
            "Epoch 462/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2307 - mae: 1.2307 - val_loss: 1.1947 - val_mae: 1.1947\n",
            "Epoch 463/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1908 - mae: 1.1908 - val_loss: 1.1178 - val_mae: 1.1178\n",
            "Epoch 464/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2104 - mae: 1.2104 - val_loss: 1.0131 - val_mae: 1.0131\n",
            "Epoch 465/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2010 - mae: 1.2010 - val_loss: 0.9532 - val_mae: 0.9532\n",
            "Epoch 466/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2736 - mae: 1.2736 - val_loss: 1.0249 - val_mae: 1.0249\n",
            "Epoch 467/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2958 - mae: 1.2958 - val_loss: 1.0182 - val_mae: 1.0182\n",
            "Epoch 468/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2367 - mae: 1.2367 - val_loss: 1.1440 - val_mae: 1.1440\n",
            "Epoch 469/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2257 - mae: 1.2257 - val_loss: 1.1980 - val_mae: 1.1980\n",
            "Epoch 470/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1727 - mae: 1.1727 - val_loss: 1.0687 - val_mae: 1.0687\n",
            "Epoch 471/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1380 - mae: 1.1380 - val_loss: 1.1620 - val_mae: 1.1620\n",
            "Epoch 472/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2557 - mae: 1.2557 - val_loss: 1.0805 - val_mae: 1.0805\n",
            "Epoch 473/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2337 - mae: 1.2337 - val_loss: 1.0376 - val_mae: 1.0376\n",
            "Epoch 474/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2284 - mae: 1.2284 - val_loss: 1.2676 - val_mae: 1.2676\n",
            "Epoch 475/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2297 - mae: 1.2297 - val_loss: 1.0864 - val_mae: 1.0864\n",
            "Epoch 476/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1891 - mae: 1.1891 - val_loss: 1.0455 - val_mae: 1.0455\n",
            "Epoch 477/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2065 - mae: 1.2065 - val_loss: 1.0837 - val_mae: 1.0837\n",
            "Epoch 478/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2359 - mae: 1.2359 - val_loss: 1.0426 - val_mae: 1.0426\n",
            "Epoch 479/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2500 - mae: 1.2500 - val_loss: 1.1345 - val_mae: 1.1345\n",
            "Epoch 480/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2499 - mae: 1.2499 - val_loss: 1.0559 - val_mae: 1.0559\n",
            "Epoch 481/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2292 - mae: 1.2292 - val_loss: 0.9759 - val_mae: 0.9759\n",
            "Epoch 482/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1630 - mae: 1.1630 - val_loss: 1.1354 - val_mae: 1.1354\n",
            "Epoch 483/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1663 - mae: 1.1663 - val_loss: 1.0667 - val_mae: 1.0667\n",
            "Epoch 484/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1558 - mae: 1.1558 - val_loss: 1.0035 - val_mae: 1.0035\n",
            "Epoch 485/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1708 - mae: 1.1708 - val_loss: 1.1816 - val_mae: 1.1816\n",
            "Epoch 486/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2043 - mae: 1.2043 - val_loss: 1.0216 - val_mae: 1.0216\n",
            "Epoch 487/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1721 - mae: 1.1721 - val_loss: 1.2589 - val_mae: 1.2589\n",
            "Epoch 488/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1917 - mae: 1.1917 - val_loss: 1.1068 - val_mae: 1.1068\n",
            "Epoch 489/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1764 - mae: 1.1764 - val_loss: 0.9464 - val_mae: 0.9464\n",
            "Epoch 490/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1930 - mae: 1.1930 - val_loss: 1.0041 - val_mae: 1.0041\n",
            "Epoch 491/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1376 - mae: 1.1376 - val_loss: 1.0897 - val_mae: 1.0897\n",
            "Epoch 492/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1996 - mae: 1.1996 - val_loss: 1.0730 - val_mae: 1.0730\n",
            "Epoch 493/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1475 - mae: 1.1475 - val_loss: 1.0274 - val_mae: 1.0274\n",
            "Epoch 494/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2070 - mae: 1.2070 - val_loss: 1.1517 - val_mae: 1.1517\n",
            "Epoch 495/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1507 - mae: 1.1507 - val_loss: 1.0149 - val_mae: 1.0149\n",
            "Epoch 496/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1865 - mae: 1.1865 - val_loss: 1.2522 - val_mae: 1.2522\n",
            "Epoch 497/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2099 - mae: 1.2099 - val_loss: 1.0254 - val_mae: 1.0254\n",
            "Epoch 498/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2470 - mae: 1.2470 - val_loss: 1.0297 - val_mae: 1.0297\n",
            "Epoch 499/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2186 - mae: 1.2186 - val_loss: 0.9390 - val_mae: 0.9390\n",
            "Epoch 500/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1733 - mae: 1.1733 - val_loss: 1.0432 - val_mae: 1.0432\n",
            "Epoch 501/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1303 - mae: 1.1303 - val_loss: 1.0191 - val_mae: 1.0191\n",
            "Epoch 502/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1519 - mae: 1.1519 - val_loss: 1.0379 - val_mae: 1.0379\n",
            "Epoch 503/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.2235 - mae: 1.2235 - val_loss: 1.1406 - val_mae: 1.1406\n",
            "Epoch 504/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1723 - mae: 1.1723 - val_loss: 1.1603 - val_mae: 1.1603\n",
            "Epoch 505/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0922 - mae: 1.0922 - val_loss: 1.2027 - val_mae: 1.2027\n",
            "Epoch 506/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1782 - mae: 1.1782 - val_loss: 1.1022 - val_mae: 1.1022\n",
            "Epoch 507/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1941 - mae: 1.1941 - val_loss: 1.0302 - val_mae: 1.0302\n",
            "Epoch 508/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1739 - mae: 1.1739 - val_loss: 0.9714 - val_mae: 0.9714\n",
            "Epoch 509/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2117 - mae: 1.2117 - val_loss: 0.9916 - val_mae: 0.9916\n",
            "Epoch 510/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1750 - mae: 1.1750 - val_loss: 0.9575 - val_mae: 0.9575\n",
            "Epoch 511/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1499 - mae: 1.1499 - val_loss: 0.8980 - val_mae: 0.8980\n",
            "Epoch 512/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2221 - mae: 1.2221 - val_loss: 1.1647 - val_mae: 1.1647\n",
            "Epoch 513/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2053 - mae: 1.2053 - val_loss: 0.9757 - val_mae: 0.9757\n",
            "Epoch 514/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1489 - mae: 1.1489 - val_loss: 1.1199 - val_mae: 1.1199\n",
            "Epoch 515/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2229 - mae: 1.2229 - val_loss: 1.0089 - val_mae: 1.0089\n",
            "Epoch 516/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1537 - mae: 1.1537 - val_loss: 1.0807 - val_mae: 1.0807\n",
            "Epoch 517/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2142 - mae: 1.2142 - val_loss: 1.0329 - val_mae: 1.0329\n",
            "Epoch 518/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2661 - mae: 1.2661 - val_loss: 0.9770 - val_mae: 0.9770\n",
            "Epoch 519/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1432 - mae: 1.1432 - val_loss: 1.0103 - val_mae: 1.0103\n",
            "Epoch 520/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1138 - mae: 1.1138 - val_loss: 0.9979 - val_mae: 0.9979\n",
            "Epoch 521/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1547 - mae: 1.1547 - val_loss: 0.9235 - val_mae: 0.9235\n",
            "Epoch 522/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1280 - mae: 1.1280 - val_loss: 0.9544 - val_mae: 0.9544\n",
            "Epoch 523/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1379 - mae: 1.1379 - val_loss: 1.0836 - val_mae: 1.0836\n",
            "Epoch 524/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0964 - mae: 1.0964 - val_loss: 0.9115 - val_mae: 0.9115\n",
            "Epoch 525/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1541 - mae: 1.1541 - val_loss: 0.9868 - val_mae: 0.9868\n",
            "Epoch 526/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1439 - mae: 1.1439 - val_loss: 1.3481 - val_mae: 1.3481\n",
            "Epoch 527/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2936 - mae: 1.2936 - val_loss: 0.9994 - val_mae: 0.9994\n",
            "Epoch 528/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0934 - mae: 1.0934 - val_loss: 0.9837 - val_mae: 0.9837\n",
            "Epoch 529/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1360 - mae: 1.1360 - val_loss: 0.9563 - val_mae: 0.9563\n",
            "Epoch 530/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1887 - mae: 1.1887 - val_loss: 1.0309 - val_mae: 1.0309\n",
            "Epoch 531/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1446 - mae: 1.1446 - val_loss: 1.0105 - val_mae: 1.0105\n",
            "Epoch 532/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1143 - mae: 1.1143 - val_loss: 1.1376 - val_mae: 1.1376\n",
            "Epoch 533/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1600 - mae: 1.1600 - val_loss: 0.9361 - val_mae: 0.9361\n",
            "Epoch 534/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1568 - mae: 1.1568 - val_loss: 0.9364 - val_mae: 0.9364\n",
            "Epoch 535/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1641 - mae: 1.1641 - val_loss: 1.1385 - val_mae: 1.1385\n",
            "Epoch 536/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1733 - mae: 1.1733 - val_loss: 1.0329 - val_mae: 1.0329\n",
            "Epoch 537/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1526 - mae: 1.1526 - val_loss: 1.0016 - val_mae: 1.0016\n",
            "Epoch 538/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.2175 - mae: 1.2175 - val_loss: 0.9753 - val_mae: 0.9753\n",
            "Epoch 539/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1306 - mae: 1.1306 - val_loss: 0.9235 - val_mae: 0.9235\n",
            "Epoch 540/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1323 - mae: 1.1323 - val_loss: 0.9217 - val_mae: 0.9217\n",
            "Epoch 541/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1243 - mae: 1.1243 - val_loss: 1.0734 - val_mae: 1.0734\n",
            "Epoch 542/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1870 - mae: 1.1870 - val_loss: 0.9190 - val_mae: 0.9190\n",
            "Epoch 543/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1014 - mae: 1.1014 - val_loss: 1.1079 - val_mae: 1.1079\n",
            "Epoch 544/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1642 - mae: 1.1642 - val_loss: 0.9707 - val_mae: 0.9707\n",
            "Epoch 545/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1834 - mae: 1.1834 - val_loss: 0.9706 - val_mae: 0.9706\n",
            "Epoch 546/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0820 - mae: 1.0820 - val_loss: 0.9663 - val_mae: 0.9663\n",
            "Epoch 547/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1100 - mae: 1.1100 - val_loss: 0.9593 - val_mae: 0.9593\n",
            "Epoch 548/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1115 - mae: 1.1115 - val_loss: 0.9048 - val_mae: 0.9048\n",
            "Epoch 549/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.1887 - mae: 1.1887 - val_loss: 0.9866 - val_mae: 0.9866\n",
            "Epoch 550/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1215 - mae: 1.1215 - val_loss: 1.0325 - val_mae: 1.0325\n",
            "Epoch 551/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0693 - mae: 1.0693 - val_loss: 1.0246 - val_mae: 1.0246\n",
            "Epoch 552/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0985 - mae: 1.0985 - val_loss: 1.0862 - val_mae: 1.0862\n",
            "Epoch 553/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1554 - mae: 1.1554 - val_loss: 1.0403 - val_mae: 1.0403\n",
            "Epoch 554/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1456 - mae: 1.1456 - val_loss: 0.9713 - val_mae: 0.9713\n",
            "Epoch 555/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0853 - mae: 1.0853 - val_loss: 0.8963 - val_mae: 0.8963\n",
            "Epoch 556/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1306 - mae: 1.1306 - val_loss: 0.9416 - val_mae: 0.9416\n",
            "Epoch 557/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1010 - mae: 1.1010 - val_loss: 0.9636 - val_mae: 0.9636\n",
            "Epoch 558/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0974 - mae: 1.0974 - val_loss: 1.0536 - val_mae: 1.0536\n",
            "Epoch 559/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0693 - mae: 1.0693 - val_loss: 0.8816 - val_mae: 0.8816\n",
            "Epoch 560/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1271 - mae: 1.1271 - val_loss: 0.9524 - val_mae: 0.9524\n",
            "Epoch 561/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1903 - mae: 1.1903 - val_loss: 1.1192 - val_mae: 1.1192\n",
            "Epoch 562/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0872 - mae: 1.0872 - val_loss: 0.9130 - val_mae: 0.9130\n",
            "Epoch 563/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1137 - mae: 1.1137 - val_loss: 0.9920 - val_mae: 0.9920\n",
            "Epoch 564/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1151 - mae: 1.1151 - val_loss: 0.9454 - val_mae: 0.9454\n",
            "Epoch 565/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1405 - mae: 1.1405 - val_loss: 0.9332 - val_mae: 0.9332\n",
            "Epoch 566/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1123 - mae: 1.1123 - val_loss: 0.8665 - val_mae: 0.8665\n",
            "Epoch 567/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1297 - mae: 1.1297 - val_loss: 0.8567 - val_mae: 0.8567\n",
            "Epoch 568/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0742 - mae: 1.0742 - val_loss: 1.0297 - val_mae: 1.0297\n",
            "Epoch 569/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1316 - mae: 1.1316 - val_loss: 0.8835 - val_mae: 0.8835\n",
            "Epoch 570/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0486 - mae: 1.0486 - val_loss: 0.8394 - val_mae: 0.8394\n",
            "Epoch 571/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0829 - mae: 1.0829 - val_loss: 1.0859 - val_mae: 1.0859\n",
            "Epoch 572/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1149 - mae: 1.1149 - val_loss: 0.9561 - val_mae: 0.9561\n",
            "Epoch 573/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1194 - mae: 1.1194 - val_loss: 1.0330 - val_mae: 1.0330\n",
            "Epoch 574/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0921 - mae: 1.0921 - val_loss: 0.9318 - val_mae: 0.9318\n",
            "Epoch 575/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0726 - mae: 1.0726 - val_loss: 1.0650 - val_mae: 1.0650\n",
            "Epoch 576/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1703 - mae: 1.1703 - val_loss: 0.9537 - val_mae: 0.9537\n",
            "Epoch 577/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0822 - mae: 1.0822 - val_loss: 0.9258 - val_mae: 0.9258\n",
            "Epoch 578/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1085 - mae: 1.1085 - val_loss: 0.9571 - val_mae: 0.9571\n",
            "Epoch 579/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1071 - mae: 1.1071 - val_loss: 1.0400 - val_mae: 1.0400\n",
            "Epoch 580/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0888 - mae: 1.0888 - val_loss: 1.0920 - val_mae: 1.0920\n",
            "Epoch 581/1000\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 1.0765 - mae: 1.0765 - val_loss: 1.0393 - val_mae: 1.0393\n",
            "Epoch 582/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1036 - mae: 1.1036 - val_loss: 1.0647 - val_mae: 1.0647\n",
            "Epoch 583/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0856 - mae: 1.0856 - val_loss: 1.0545 - val_mae: 1.0545\n",
            "Epoch 584/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1423 - mae: 1.1423 - val_loss: 0.8842 - val_mae: 0.8842\n",
            "Epoch 585/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0521 - mae: 1.0521 - val_loss: 0.8487 - val_mae: 0.8487\n",
            "Epoch 586/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1474 - mae: 1.1474 - val_loss: 0.9556 - val_mae: 0.9556\n",
            "Epoch 587/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1178 - mae: 1.1178 - val_loss: 0.9068 - val_mae: 0.9068\n",
            "Epoch 588/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1123 - mae: 1.1123 - val_loss: 0.9451 - val_mae: 0.9451\n",
            "Epoch 589/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0899 - mae: 1.0899 - val_loss: 0.9837 - val_mae: 0.9837\n",
            "Epoch 590/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0679 - mae: 1.0679 - val_loss: 1.0934 - val_mae: 1.0934\n",
            "Epoch 591/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0442 - mae: 1.0442 - val_loss: 1.0834 - val_mae: 1.0834\n",
            "Epoch 592/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1326 - mae: 1.1326 - val_loss: 0.9778 - val_mae: 0.9778\n",
            "Epoch 593/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1130 - mae: 1.1130 - val_loss: 0.9182 - val_mae: 0.9182\n",
            "Epoch 594/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1742 - mae: 1.1742 - val_loss: 0.9874 - val_mae: 0.9874\n",
            "Epoch 595/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0418 - mae: 1.0418 - val_loss: 0.9919 - val_mae: 0.9919\n",
            "Epoch 596/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1239 - mae: 1.1239 - val_loss: 0.9697 - val_mae: 0.9697\n",
            "Epoch 597/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0680 - mae: 1.0680 - val_loss: 0.8306 - val_mae: 0.8306\n",
            "Epoch 598/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1375 - mae: 1.1375 - val_loss: 1.0020 - val_mae: 1.0020\n",
            "Epoch 599/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0749 - mae: 1.0749 - val_loss: 0.9328 - val_mae: 0.9328\n",
            "Epoch 600/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0793 - mae: 1.0793 - val_loss: 0.9492 - val_mae: 0.9492\n",
            "Epoch 601/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1176 - mae: 1.1176 - val_loss: 0.8874 - val_mae: 0.8874\n",
            "Epoch 602/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1094 - mae: 1.1094 - val_loss: 1.1455 - val_mae: 1.1455\n",
            "Epoch 603/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1322 - mae: 1.1322 - val_loss: 0.9600 - val_mae: 0.9600\n",
            "Epoch 604/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0826 - mae: 1.0826 - val_loss: 0.9157 - val_mae: 0.9157\n",
            "Epoch 605/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0571 - mae: 1.0571 - val_loss: 0.8974 - val_mae: 0.8974\n",
            "Epoch 606/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1240 - mae: 1.1240 - val_loss: 1.0424 - val_mae: 1.0424\n",
            "Epoch 607/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1513 - mae: 1.1513 - val_loss: 1.0885 - val_mae: 1.0885\n",
            "Epoch 608/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1708 - mae: 1.1708 - val_loss: 0.9186 - val_mae: 0.9186\n",
            "Epoch 609/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0501 - mae: 1.0501 - val_loss: 0.9279 - val_mae: 0.9279\n",
            "Epoch 610/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0116 - mae: 1.0116 - val_loss: 0.9154 - val_mae: 0.9154\n",
            "Epoch 611/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0629 - mae: 1.0629 - val_loss: 1.0401 - val_mae: 1.0401\n",
            "Epoch 612/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0943 - mae: 1.0943 - val_loss: 0.8822 - val_mae: 0.8822\n",
            "Epoch 613/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1893 - mae: 1.1893 - val_loss: 1.0382 - val_mae: 1.0382\n",
            "Epoch 614/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0659 - mae: 1.0659 - val_loss: 1.0535 - val_mae: 1.0535\n",
            "Epoch 615/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1136 - mae: 1.1136 - val_loss: 1.0668 - val_mae: 1.0668\n",
            "Epoch 616/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1258 - mae: 1.1258 - val_loss: 0.9681 - val_mae: 0.9681\n",
            "Epoch 617/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0576 - mae: 1.0576 - val_loss: 0.8682 - val_mae: 0.8682\n",
            "Epoch 618/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0538 - mae: 1.0538 - val_loss: 0.9115 - val_mae: 0.9115\n",
            "Epoch 619/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0847 - mae: 1.0847 - val_loss: 1.0302 - val_mae: 1.0302\n",
            "Epoch 620/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0225 - mae: 1.0225 - val_loss: 0.9821 - val_mae: 0.9821\n",
            "Epoch 621/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0877 - mae: 1.0877 - val_loss: 0.9711 - val_mae: 0.9711\n",
            "Epoch 622/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0867 - mae: 1.0867 - val_loss: 1.0349 - val_mae: 1.0349\n",
            "Epoch 623/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.0520 - mae: 1.0520 - val_loss: 0.9818 - val_mae: 0.9818\n",
            "Epoch 624/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0875 - mae: 1.0875 - val_loss: 0.8787 - val_mae: 0.8787\n",
            "Epoch 625/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1139 - mae: 1.1139 - val_loss: 1.0109 - val_mae: 1.0109\n",
            "Epoch 626/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0665 - mae: 1.0665 - val_loss: 0.9412 - val_mae: 0.9412\n",
            "Epoch 627/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1361 - mae: 1.1361 - val_loss: 0.8345 - val_mae: 0.8345\n",
            "Epoch 628/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.1364 - mae: 1.1364 - val_loss: 1.0319 - val_mae: 1.0319\n",
            "Epoch 629/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1022 - mae: 1.1022 - val_loss: 0.9308 - val_mae: 0.9308\n",
            "Epoch 630/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0933 - mae: 1.0933 - val_loss: 0.9315 - val_mae: 0.9315\n",
            "Epoch 631/1000\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 1.1153 - mae: 1.1153 - val_loss: 0.9154 - val_mae: 0.9154\n",
            "Epoch 632/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0321 - mae: 1.0321 - val_loss: 0.9731 - val_mae: 0.9731\n",
            "Epoch 633/1000\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 1.0516 - mae: 1.0516 - val_loss: 0.8739 - val_mae: 0.8739\n",
            "Epoch 634/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.0318 - mae: 1.0318"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-47579962e87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUo3AxhLqO81",
        "colab_type": "code",
        "outputId": "f755204f-8bc0-47c3-baba-681c3f1762b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "model.evaluate(x_eval, y_eval)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "532/532 [==============================] - 1s 3ms/step - loss: 0.8609 - mae: 0.8609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8609458804130554, 0.8609458804130554]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJYUw-Q3qPqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(x_eval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8TskHM9qZOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test = model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orzGNwdKqg0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
